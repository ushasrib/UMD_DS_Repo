{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c010436e",
   "metadata": {},
   "source": [
    "#### 04/16/24 update: In this notebook, building on sourcecode, \n",
    "- created a mapping dataframe with q_id, source_name and response_type\n",
    "- merged it with survey_questions dataframe to add response_types to the existing postgres table. The join condition is q_id and source_name combination.\n",
    "\n",
    "**Summary**\n",
    "- In this edition, of the 1302 unique response types created from value labels, not all got updated in survey_questions table as survey_questions table q_ids are based on variable labels and mapping_df q_ids are based on value labels. There exist some questions in variable labels that have no response types assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b733dc58",
   "metadata": {
    "id": "b733dc58",
    "outputId": "03295d28-6696-4f5a-ac1b-ac9eb2501313"
   },
   "outputs": [],
   "source": [
    "#This code picks up question ids and corresponding range of responses using pandas value_labels() method on stata and spss files\n",
    "#It then creates Unique identifiers for response types by grouping similar response_types\n",
    "#It also creates a dataframe with all the Question to reponse_type mappings \n",
    "\n",
    "import pandas as pd\n",
    "import pyreadstat\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "#Defining functions to return value labels from stata and spss files\n",
    "def get_value_labels_stata(source_file):\n",
    "    stata_iterator = pd.read_stata(source_file, iterator=True)\n",
    "    return stata_iterator.value_labels()\n",
    "\n",
    "def get_value_labels_spss(source_file):\n",
    "    df, meta = pyreadstat.read_sav(source_file)\n",
    "    return meta.variable_value_labels\n",
    "\n",
    "#Our 9 source files downloaded from different barometers, source names derived from source files\n",
    "source_files = {\"WVS_Wave_7_United_States_Stata_v5.0.dta\":\"WVS_Wave_7_United_States_Stata_v5.0.dta\",\n",
    "                \"Latinobarometro_2023_Eng_Stata_v1_0.dta\":\"Latinobarometro_2023_Eng_Stata_v1_0.dta\",\n",
    "                \"ZA7781_v2-0-0.dta\": \"Eurobarometer_v2-0-0.dta\",\n",
    "               \"AB7_ENG_Release_Version6.dta\": \"Arab_Barometer_ENG_Release_Version6.dta\",\n",
    "               \"USA_2023_LAPOP_AmericasBarometer_v1.0_w.dta\": \"USA_2023_LAPOP_AmericasBarometer_v1.0_w.dta\",\n",
    "               \"Caucasus_CB_2017_Georgia_public_17.11.17.dta\":\"Caucasus_CB_2017_Georgia_public_17.11.17.dta\",\n",
    "              \"central-asia-barometer-survey-wave-1-stata-kyrgyzstan-2017-spring.dta\":\"central-asia-barometer-survey-wave-1-stata-kyrgyzstan-2017-spring.dta\",\n",
    "              \"SAF_R9.data_.final_.wtd_release.30May23.sav\":\"SouthAfrica.data_30May23.sav\",\n",
    "              \"20230504_W5_merge_15.dta\": \"All_Asian_Countries_W5_merge_15.dta\"}\n",
    "\n",
    "number_of_files = len(source_files)\n",
    "response_types = {}\n",
    "num_response_types_total = 0\n",
    "unique_response_types = set()\n",
    "count_q_id = 0\n",
    "\n",
    "q_id_mapping_list = []\n",
    "\n",
    "for source_file, source_name in source_files.items():\n",
    "    # Assigning value labels from different source files to a variable\n",
    "    if source_file.endswith('.dta'):\n",
    "\n",
    "        x = get_value_labels_stata(source_file)\n",
    "        df_source_file = pd.read_stata(source_file, convert_categoricals=False)\n",
    "        df_source_file.columns = df_source_file.columns.str.lower()\n",
    "\n",
    "    elif source_file.endswith('.sav'):\n",
    "\n",
    "        x =  get_value_labels_spss(source_file)\n",
    "        df_source_file, meta = pyreadstat.read_sav(source_file)\n",
    "        df_source_file.columns = df_source_file.columns.str.lower()\n",
    "    \n",
    "    # Counting number of response types for the current source file\n",
    "    num_response_types = len(x)\n",
    "    num_response_types_total += num_response_types\n",
    "\n",
    "    # Printing number of response types in the current source file\n",
    "    print(f\"Number of response types in {source_name}: {num_response_types}\")\n",
    "    questions_source_file =[]\n",
    "    # Iterating through the value labels dictionary obtaining q_ids and answers\n",
    "    for q_id, answers in x.items():\n",
    "        q_id = q_id.lower()\n",
    "       \n",
    "        count_q_id += 1\n",
    "        response_type = None\n",
    "        #checking if answers already exist in the response_types dict and if they do, assigning to a pre-existing rt_id\n",
    "        for rt_id, rt_answers in response_types.items():\n",
    "            if rt_answers['answers'] == answers:\n",
    "                response_type = rt_id\n",
    "                break\n",
    "\n",
    "        # If response type doesn't exist, creating a new one\n",
    "        if response_type is None:\n",
    "            response_type = 'RT{}'.format(len(response_types)+1)\n",
    "            response_types[response_type] = {\n",
    "                'answers': answers,\n",
    "                'source_name': source_name,\n",
    "                'inferred_RT': 0  \n",
    "            }\n",
    "            \n",
    "        # Adding (q_id, source_name, response_type tuple) to the mapping list\n",
    "        q_id_mapping_list.append((q_id, source_name, response_type))\n",
    "        questions_source_file.append(q_id)\n",
    "        #print(\"Question {} from source_name {} is assigned to response type {}\".format(q_id, source_name, response_type))\n",
    "        \n",
    "    df_source_file_columns = df_source_file.columns.str.lower()\n",
    "    for column in df_source_file_columns:\n",
    "        if all(column != q_id for q_id in questions_source_file):\n",
    "            unique_values = df_source_file[column].unique().tolist()\n",
    "\n",
    "            # Create a dictionary encoding the values with numerical keys\n",
    "            value_encoding = {i+1: val for i, val in enumerate(unique_values)}\n",
    "            response_type = 'RT{}'.format(len(response_types)+1)\n",
    "            response_types[response_type] = {\n",
    "            'answers': value_encoding,\n",
    "            'source_name': source_name,\n",
    "            'inferred_RT': 1  \n",
    "                }\n",
    "            \n",
    "            q_id_mapping_list.append((column, source_name, response_type))    \n",
    "\n",
    "        # Add to unique_response_types set\n",
    "        unique_response_types.add(response_type)\n",
    "        \n",
    "#Creating a mapping dataframe to merge with survey_questions dataframe\n",
    "mapping_df = pd.DataFrame(q_id_mapping_list, columns = ['q_id', 'source_name', 'response_type'])\n",
    "\n",
    "print(count_q_id)\n",
    "print(f'Created a mapping dataframe with {len(mapping_df)} q_id to response_types mapping from {number_of_files} files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947eee18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9c6d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#response_types\n",
    "mapping_df[mapping_df['response_type']=='RT313']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd5340e",
   "metadata": {
    "id": "edd5340e",
    "outputId": "88108baa-9173-4bcc-de6d-5a5a2318a7fe",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#This code creates the dimension_value table with response type, label, code and source name\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "rows = []\n",
    "\n",
    "\n",
    "# Iterating over response_types dictionary created in previous code\n",
    "for response_type, data in response_types.items():\n",
    "    answers = data['answers']\n",
    "    source_name = data['source_name']\n",
    "    inferred_RT = data['inferred_RT']\n",
    "\n",
    "        \n",
    "    # Iterating over each response and its value\n",
    "    for response_code, response_label in answers.items():\n",
    "        row = {\n",
    "            'response_type': response_type,\n",
    "            'response_label': response_label,\n",
    "            'response_code': response_code,\n",
    "            'source_name': source_name,\n",
    "            'inferred_RT': inferred_RT\n",
    "          \n",
    "        }\n",
    "        # Appendings rows to rows list\n",
    "        rows.append(row)\n",
    "    \n",
    "\n",
    "# Creating a pandas DataFrame from the list of rows\n",
    "dimension_value_df = pd.DataFrame(rows)\n",
    "\n",
    "#Creating dimension_value postgres table from dataframe\n",
    "connection_str = 'postgresql://postgres:Capstone@localhost/ics_capstone'\n",
    "engine = create_engine(connection_str)\n",
    "table_name = 'dimension_value'\n",
    "\n",
    "dimension_value_df.to_sql(table_name, engine, if_exists='replace', index=False)\n",
    "print(f'Created dimension_value table in postgres database with {len(dimension_value_df)} rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f9295d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This code extracts variable labels from stata and spss files to create a survey questions dataframe\n",
    "# The mapping dataframe created in previous code is merged with the survey questions data frame\n",
    "#survey_questions table is created from the merged dataframes\n",
    "\n",
    "import pandas as pd\n",
    "import pyreadstat\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "source_files = {\"WVS_Wave_7_United_States_Stata_v5.0.dta\":\"WVS_Wave_7_United_States_Stata_v5.0.dta\",\n",
    "                \"Latinobarometro_2023_Eng_Stata_v1_0.dta\":\"Latinobarometro_2023_Eng_Stata_v1_0.dta\",\n",
    "                \"ZA7781_v2-0-0.dta\": \"Eurobarometer_v2-0-0.dta\",\n",
    "               \"AB7_ENG_Release_Version6.dta\": \"Arab_Barometer_ENG_Release_Version6.dta\",\n",
    "               \"USA_2023_LAPOP_AmericasBarometer_v1.0_w.dta\": \"USA_2023_LAPOP_AmericasBarometer_v1.0_w.dta\",\n",
    "               \"Caucasus_CB_2017_Georgia_public_17.11.17.dta\":\"Caucasus_CB_2017_Georgia_public_17.11.17.dta\",\n",
    "              \"central-asia-barometer-survey-wave-1-stata-kyrgyzstan-2017-spring.dta\":\"central-asia-barometer-survey-wave-1-stata-kyrgyzstan-2017-spring.dta\",\n",
    "              \"SAF_R9.data_.final_.wtd_release.30May23.sav\":\"SouthAfrica.data_30May23.sav\",\n",
    "              \"20230504_W5_merge_15.dta\": \"All_Asian_Countries_W5_merge_15.dta\"}\n",
    "\n",
    "number_of_files = len(source_files)\n",
    "\n",
    "#Creating functions to return variable labels\n",
    "def get_variable_labels_stata(source_file):\n",
    "    stata_iterator = pd.read_stata(source_file, iterator=True)\n",
    "    return stata_iterator.variable_labels()\n",
    "\n",
    "def get_variable_labels_spss(source_file):\n",
    "    df, meta = pyreadstat.read_sav(source_file)\n",
    "    variable_labels = {}\n",
    "    for column_name, label in zip(meta.column_names, meta.column_labels):\n",
    "        variable_labels[column_name] = label\n",
    "    return variable_labels\n",
    "\n",
    "rows = []\n",
    "total_rows = 0\n",
    "\n",
    "for source_file, source_name in source_files.items():\n",
    "    # assigning variable labels from the source file to a variable\n",
    "    if source_file.endswith('.dta'):\n",
    "        x = get_variable_labels_stata(source_file)\n",
    "    elif source_file.endswith('.sav'):\n",
    "        x = get_variable_labels_spss(source_file)\n",
    "    num_rows = len(x)    \n",
    "    print(f\"Length of variable labels for {source_name}: {num_rows}\")\n",
    "    total_rows += num_rows\n",
    "        \n",
    "    # Extracting variable labels and appending them to the rows list\n",
    "    for q_id, q_text in x.items():\n",
    "        q_id = q_id.lower()\n",
    "        row = {\n",
    "               'q_id': q_id,\n",
    "               'q_text': q_text,\n",
    "               'source_name': source_name\n",
    "                }\n",
    "        rows.append(row)\n",
    "\n",
    "# Creating an initial DataFrame from the list of rows\n",
    "survey_questions_df = pd.DataFrame(rows)\n",
    "duplicate_count_sq = survey_questions_df.duplicated(subset=['q_id']).sum()\n",
    "print(\"Number of duplicate q_ids in survey_questions table are:\", duplicate_count_sq)\n",
    "\n",
    "# null_counts_sq = survey_questions_df.isnull().sum()\n",
    "# print(f'Nulls in survey_questions_df are {null_counts_sq}')\n",
    "\n",
    "\n",
    "#merging mapping_df from previous code to include response_types\n",
    "sq_rt_df = pd.merge(survey_questions_df, mapping_df, on=['q_id','source_name'], how='left')\n",
    "print(f'No of rows in merged dataframe sq_rt_df is {len(sq_rt_df)}')\n",
    "# null_counts_sq_rt = sq_rt_df.isnull().sum()\n",
    "# print(f'Nulls in merged sq_rt_df are {null_counts_sq_rt}')\n",
    "\n",
    "\n",
    "\n",
    "#Creating survey_questions postgres table from merged dataframe\n",
    "connection_string = 'postgresql://postgres:Capstone@localhost/ics_capstone'\n",
    "\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "table_name = 'survey_questions'\n",
    "\n",
    "# Inserting the DataFrame into the PostgreSQL table\n",
    "sq_rt_df.to_sql(table_name, engine, if_exists='replace', index=False)\n",
    "\n",
    "\n",
    "print(f\"Out of {total_rows} rows from {number_of_files} files, inserted {len(sq_rt_df)} rows into the '{table_name}' table in postgres database.\")\n",
    "#print(f'There are {null_counts_sq_rt} Nulls in the response_types column in the survey_questions table as q_ids are not the same in value labels and variable labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35458ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code finds number of unmatched **q_ids only**from mapping_df and survey_questions dataframe\n",
    "import pandas as pd\n",
    "\n",
    "#unmatched_survey_qids = set(survey_questions_df[survey_questions_df[['q_id']].lower()) - set(mapping_df[mapping_df[['q_id']].lower())\n",
    "unmatched_survey_qids = set(survey_questions_df['q_id']) - set(mapping_df['q_id'])\n",
    "print(f'Number of unmatched q_ids in survey_questions dataframe are: {len(unmatched_survey_qids)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bfa352",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code updates the actual survey responses from respondents into a postgres table\n",
    "#If we ran this code for all the files, we are receiving memory error in our laptops. \n",
    "#Therefore, we ran this code for one stata and one spss files and created the table successfully\n",
    "\n",
    "source_files = {\n",
    "    \"WVS_Wave_7_United_States_Stata_v5.0.dta\": \"WVS_Wave_7_United_States_Stata_v5.0.dta\",\n",
    "    \"SAF_R9.data_.final_.wtd_release.30May23.sav\": \"SouthAfrica.data_30May23.sav\"   \n",
    "    \n",
    "                }\n",
    "# \"Latinobarometro_2023_Eng_Stata_v1_0.dta\": \"Latinobarometro_2023_Eng_Stata_v1_0.dta\",\n",
    "#     \"ZA7781_v2-0-0.dta\": \"Eurobarometer_v2-0-0.dta\",\n",
    "#     \"AB7_ENG_Release_Version6.dta\": \"Arab_barometer_ENG_Release_Version6.dta\",\n",
    "#     \"USA_2023_LAPOP_AmericasBarometer_v1.0_w.dta\": \"USA_2023_LAPOP_AmericasBarometer_v1.0_w.dta\",\n",
    "#     \"Caucasus_CB_2017_Georgia_public_17.11.17.dta\": \"Caucasus_CB_2017_Georgia_public_17.11.17.dta\",\n",
    "#     \"central-asia-barometer-survey-wave-1-stata-kyrgyzstan-2017-spring.dta\": \"central-asia-barometer-survey-wave-1-stata-kyrgyzstan-2017-spring.dta\",\n",
    "#     \"SAF_R9.data_.final_.wtd_release.30May23.sav\": \"SouthAfrica.data_30May23.sav\",\n",
    "#     \"20230504_W5_merge_15.dta\": \"All_Asian_Countries_W5_merge_15.dta\"\n",
    "\n",
    "# Create an empty list to store the data\n",
    "data = []\n",
    "\n",
    "# Creating dataframes with actual survey responses from stata and spss tables\n",
    "for source_file, source_name in source_files.items():\n",
    "   \n",
    "    if source_file.endswith('.dta'):\n",
    "\n",
    "        df = pd.read_stata(source_file, convert_categoricals=False)\n",
    "\n",
    "    elif source_file.endswith('.sav'):\n",
    "\n",
    "        df, meta =  pyreadstat.read_sav(source_file)\n",
    "       \n",
    "    \n",
    "    # Iterating through the rows of the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        for column in df.columns:\n",
    "                q_id = column\n",
    "                q_id = q_id.lower()\n",
    "                response = row[column]\n",
    "                respondent_id = index+1  # Using the index as the respondent_id\n",
    "\n",
    "                data.append({'respondent_id': respondent_id, 'q_id': q_id, 'response': response, 'source_name': source_name})\n",
    "           \n",
    "df_combined = pd.DataFrame(data)\n",
    "print(df_combined.shape)\n",
    "connection_str = 'postgresql://postgres:Capstone@localhost/ics_capstone'\n",
    "engine = create_engine(connection_str)\n",
    "table_name = 'response_values'        \n",
    "df_combined.to_sql(table_name, engine, if_exists='replace', index=False)\n",
    "print(f\"Created the {table_name} table in postgres with {len(df_combined)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b24ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f925f986",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
